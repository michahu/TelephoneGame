# Experiment stimuli

* In the first run of our experiment (June 2021), we obtained ratings for short (12-token), medium (21-token), and long (37-token) sentences sampled from wikipedia as well as short, medium, and long sentences from an (inconsistent) pseudo-Gibbs algorithm. We then obtained ratings for short (length 12) sentences generated by an ngram model and lstm model.

* In the second run of our experiment (November 2021), we included sentences from a consistent pseudo-Gibbs sampling algorithm derived from the Generative Stochastic Network (GSN, Bengio, 2013) framework, as well as sentences sampled using a Metropolis-Hastings algorithm (Goyal, Dyer, Berg-Kirkpatrick, 2021). 