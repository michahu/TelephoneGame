---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(tidyboot)
library(ggthemes)
library(lme4)
library(lmerTest)
library(gganimate)
library(gifski)
library(caret)
```

# Import & clean data

we exclude the expeirmenter's ID to make sure there's no data from sandboxing

```{r}
d.raw <- read_csv('../data/dataFromMongo.csv') %>%
  filter(!is.na(wID)) %>%
  filter(wID != '60061239e01afe9652f17412') %>%
  mutate(response = as.numeric(response),
         normed_response = scale(response),
         step = ifelse(is.na(count), step, count),
         source = ifelse(source == 'gibbs', 'GSN', source),
         source = ifelse(source == 'bert', 'gibbs', source))
```

we exclude people who failed the catch trials

```{r}
failed_catch_trial1 <- d.raw %>%
  filter(stimulus_type %in% c('catch1')) %>%
  mutate(fail = response > 50) %>%
  filter(fail) %>%
  pull(wID) %>%
  unique()

failed_catch_trial2 <- d.raw %>%
  filter(stimulus_type %in% c('catch2'), iterationName == 'prolific-batch3-full') %>%
  mutate(fail = response < 50) %>%
  filter(fail) %>%
  pull(wID) %>%
  unique()


d <- d.raw %>%
  filter(stimulus_type == 'main_judgement') %>%
  group_by(wID, sent_length) %>%
  filter(!(wID %in% c(failed_catch_trial1, failed_catch_trial2))) 

d %>%
  group_by(step, phase, sent_length, source, prob, prob_class, sentence, sentence_num, id) %>%
  summarize(avg_response = mean(response)) %>%
  arrange(-avg_response) %>%
  rename(initial_state_prob_class = prob_class) %>%
  unite(sentence_id, sentence_num, id) %>%
  write_csv('../data/averaged_responses.csv')
```

Print some basic properties of our dataset.

```{r}
trials_per_participant <- d %>%
  group_by(wID) %>%
  tally()  

cat('we have data from', length(trials_per_participant$wID), 'uniqe participants\n')
cat('for', d$sentence %>% unique() %>% length(), 'uniqe sentences\n')
cat('and an average of', d %>% group_by(sentence) %>% tally() %>% ungroup() %>% summarize(m = mean(n)) %>% pull(m), 'responses per sentence')
```

Let's look at how many sentences we have of each type.

```{r}
d %>%
  group_by(source, phase, step, sentence) %>%
  tally() %>%
  mutate(step = ifelse(step > 1000, 1000, step)) %>%
  group_by(source, phase) %>%
  tally() 
```

Look at overall naturalness ratings for each sentence type.

```{r}
d %>%
  group_by(sent_length, source) %>%
  filter(phase != 'early') %>%
  filter(source != 'gibbs') %>%
  tidyboot_mean(as.numeric(response), nboot = 1000, na.rm = F)  %>%
  ungroup() %>%
  mutate(source = fct_relevel(source, 'wiki', 'GSN', 'gibbs', 'MH', 'lstm', 'ngram'),
         sent_length = fct_relevel(sent_length, 'short', 'medium', 'long')) %>%
  ggplot(aes(x = source, y = empirical_stat, fill = source)) +
    geom_bar(stat = 'identity', position = 'dodge') +
    geom_hline(yintercept = 50, linetype = 'dotted') +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = 'dodge', 
                  color = 'grey40', width = 0) +
    scale_fill_colorblind() +
    ylim(0, 100) +
    facet_grid(~ sent_length, scales = 'free_x', space = 'free_x') +
    labs(y = "mean naturalness", x = '') +
    guides(fill = 'none') +
    cowplot::theme_cowplot() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

ggsave('./empirical.pdf', width = 4, height = 3, units = 'in')
```

```{r}
library(lme4)
library(xtable)
library(lmerTest)
library(broom.mixed)
d %>%
  filter(phase != 'early') %>%
  ungroup() %>%
  mutate(source = fct_relevel(source, 'GSN')) %>%
  mutate(sent_length = fct_relevel(sent_length, 'short')) %>%
  lm(response ~ sent_length * source,
       data = .) %>%
  summary()
  tidy() %>%
  xtable() 
```

# Examine burn-in phase

```{r}
d %>%
  filter(source %in% c('GSN', 'wiki')) %>%
  mutate(step = ifelse(phase == 'late', 0, step)) %>%
  filter(step < 1000) %>%
  group_by(prob_class, sent_length, step, phase, source) %>%
  tidyboot_mean(response, nboot = 100, na.rm = F) %>%
  group_by(source, phase) %>%
  pivot_wider(names_from = c('source', 'phase'), 
              values_from = c('n', 'mean', 'empirical_stat', 'ci_lower', 'ci_upper')) %>%
  group_by(prob_class, sent_length) %>%
  mutate(empirical_stat_wiki = mean(empirical_stat_wiki_wiki, na.rm = TRUE),
         ci_lower_wiki = mean(ci_lower_wiki_wiki, na.rm = TRUE),
         ci_upper_wiki = mean(ci_upper_wiki_wiki, na.rm = TRUE),
         ci_lower_GSN_late = mean(ci_lower_GSN_late, na.rm = TRUE),
         ci_upper_GSN_late = mean(ci_upper_GSN_late, na.rm = TRUE),
         empirical_stat_GSN_late = mean(empirical_stat_GSN_late, na.rm = TRUE)) %>%
  ggplot(aes(x = log1p(step), y = empirical_stat_GSN_early)) +
    geom_point() +
    facet_grid(prob_class ~ sent_length) +
    geom_hline(aes(yintercept = empirical_stat_wiki), fill = 'green') +
    geom_errorbar(aes(ymin = ci_lower_GSN_early, ymax = ci_upper_GSN_early), width = 0) +
    geom_ribbon(aes(ymin = ci_lower_wiki, ymax = ci_upper_wiki), fill = 'green', alpha = 0.1) +
    geom_ribbon(aes(ymin = ci_lower_GSN_late, ymax = ci_upper_GSN_late), alpha = 0.1) +
    geom_hline(aes(yintercept = empirical_stat_GSN_late)) +
    geom_smooth(method = 'lm',  se = F, span = 2) +
    theme_few()

ggsave('./burn-in.pdf')
```
Statistics for burn-in.

```{r}
d %>%
  filter(source == 'bert') %>%
  filter(step < 1000) %>%
  lmer(response ~ poly(log1p(step), 2)   * prob_class + sent_length + (1  | wID),
       contrasts = list(sent_length = contr.sum(3),
                        prob_class = contr.sum(2)),
       data = .) %>%
  summary()
```

# Predict human judgements from features

```{r}
d.features <- read_csv('../data/EncodingModel/features.csv') %>%
  mutate(num_chars = str_length(sentence), nADV = str_count(pos_list, "'ADV'"),
         nPRON = str_count(pos_list, "'PRON'"), nAUX = str_count(pos_list, "'AUX'"),
         nADJ = str_count(pos_list, "'ADJ'"), nNOUN = str_count(pos_list, "'NOUN'"),
         nCCONJ = str_count(pos_list, "'CCONJ'"), nDET = str_count(pos_list, "'DET'"), 
         nPUNCT = str_count(pos_list, "'PUNCT'"), nVERB = str_count(pos_list, "'VERB'"), 
         nNUM = str_count(pos_list, "'NUM'"), nPROPN = str_count(pos_list, "'PROPN'"), 
         nPART = str_count(pos_list, "'PART'"), nADP = str_count(pos_list, "'ADP'"), 
         nSCONJ = str_count(pos_list, "'SCONJ'"), nINTJ = str_count(pos_list, "'INTJ'"), 
        acl = str_count(dep_list, "'acl'"), 
         acomp = str_count(dep_list, "'acomp'"),
         advcl = str_count(dep_list, "'advcl'"),
         advmod = str_count(dep_list, "'advmod'"),
             agent= str_count(dep_list, "'agent'"),
         amod = str_count(dep_list, "'amod'"), 
        appos = str_count(dep_list, "'appos'"),
          attr = str_count(dep_list, "'attr'"),
         auxpass = str_count(dep_list, "'auxpass'"), 
          aux = str_count(dep_list, "'aux'"),
          case = str_count(dep_list, "'case'"),
          cc = str_count(dep_list, "'cc'"),
         ccomp = str_count(dep_list, "'ccomp'"),
         conj = str_count(dep_list, "'conj'"), csubj = str_count(dep_list, "'csubj'"), 
          compound = str_count(dep_list, "'compound'"), csubjpass = str_count(dep_list, "'csubjpass'"), 
        dative = str_count(dep_list, "'dative'"),dep = str_count(dep_list, "'dep'"), 
        det = str_count(dep_list, "'det'"), dobj = str_count(dep_list, "'dobj'"), 
        expl = str_count(dep_list, "'expl'"),
          intj = str_count(dep_list, "'intj'"),
          mark = str_count(dep_list, "'mark'"),   meta = str_count(dep_list, "'meta'"),
             neg= str_count(dep_list, "'neg'"), nounmod= str_count(dep_list, "'nounmod'"), 
          npmod = str_count(dep_list, "'npmod'"),
          nsubj= str_count(dep_list, "'nsubj'"),  
         nsubjpass = str_count(dep_list, "'nsubjpass'"), 
      nummod = str_count(dep_list, "'nummod'"), 
           oprd= str_count(dep_list, "'oprd'"),
         parataxis = str_count(dep_list, "'parataxis'"), pcomp = str_count(dep_list, "'pcomp'"),
          pobj = str_count(dep_list, "'pobj'"),  poss = str_count(dep_list, "'poss'"), 
        preconj = str_count(dep_list, "'preconj'"),  predet = str_count(dep_list, "'predet'"), 
      prep = str_count(dep_list, "'prep'"),
         # prt = str_count(dep_list, "'prt'"),
            punct = str_count(dep_list, "'punct'"),
           xcomp = str_count(dep_list, "'xcomp'"),  
             relcl = str_count(dep_list, "'relcl'"), 
             quantmod= str_count(dep_list, "'quantmod'")
        )  %>%
  filter(source %in% c('bert', 'wiki')) %>%
  select(-step, -initial_state_prob_class, -sentence, -sentence_id, -pos_list, -tag_list, -dep_list)
```

First, let's restrict ourselves to just the converged BERT sentences. in addition to probability, it looks like the use of `npobj', and `nparataxis' helped?

```{r message =F }
train.control <- trainControl(method = "cv", number = 10)

# Train the model
step.model <- train(avg_response ~ ., data = d.features %>%
                      select(-source, -phase),
                    method = "lmStepAIC", 
                    #tuneGrid = data.frame(nvmax = 1:38),
                    trControl = train.control)
best.lm <- step.model$finalModel
dat <- step.model$trainingData
```

```{r}
best.lm %>%  summary()
```

Now let's look at the full dataset and see how much variance is accounted for by  corpus after controlling for all of these features?

```{r}
best.lm
more.best.lm = update(best.lm,~ . + source, data = d.features %>%
  mutate(`.outcome` = avg_response, 
         sent_lengthmedium = as.numeric(sent_length == 'medium'), 
         sent_lengthshort = as.numeric(sent_length == 'short')))

anova(best.lm, more.best.lm, test = 'Rao')
```

# Supplemental

## Let's show variation across sentences?

```{r}
d %>%
  filter(source %in% c('bert')) %>%
  filter(step < 999) %>%
  group_by(step, sentence, sent_length, prob_class, sentence_num, id) %>%
  summarize(response = mean(response)) %>%
  ggplot(aes(x = log1p(step), y = response)) +
    geom_jitter(alpha = 0.1, height = 0, width = 0.2) +
    geom_smooth(method = 'lm', formula = y ~ poly(x, 2), color = 'black') +
    facet_grid(prob_class ~ sent_length) +
    theme_few() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggsave('./item-level-variation.pdf')
```


```{r}
d %>%
  filter(source %in% c('bert', 'wiki')) %>%
  filter(step != 999) %>%
  unite(source, source, phase) %>%
  mutate(step = ifelse(step > 998, 1000, step),
         step = ifelse(source == 'wiki_wiki', -1, step),
         step = ifelse(source == 'bert_late', 10000, step)) %>%
  ggplot(aes(x = factor(step), y = response, color = source)) +
    geom_jitter(alpha = 0.05, width = 0.2) +
    geom_boxplot(alpha = 0.8) +
    #geom_smooth(se = F, method = 'lm', span = 2) +
    facet_grid(sent_length ~ prob_class) +
    theme_few() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r}
d.cors <- d %>%
    filter(source %in% c('wiki', 'bert')) %>%
    group_by(source, prob, sentence, sent_length, sentence_num, id) %>%
    summarize(m = mean(response)) %>% 
    group_by(source, sent_length) %>%
    summarize(c = cor(prob, m, method = 'spearman'))
```

```{r}
library(broom)
d.cors <- d %>%
    filter(source %in% c('wiki', 'bert')) %>%
    #filter(prob > -200) %>%
    group_by(source, prob, sentence, sent_length, sentence_num, id) %>%
    summarize(m = mean(response)) %>% 
    group_by(source, sent_length) %>%
    group_modify(~ {cor.test(.$prob, .$m, method = 'spearman') %>% tidy()}) %>%
    mutate(r = paste0('r = ', round(estimate, 2), ifelse(p.value < 0.01, '**', ' (n.s.)')))

d %>%
  filter(source %in% c('bert', 'wiki')) %>%
  filter(prob > -200) %>%
  group_by(source, prob, sent_length, sentence_num, id) %>%
  summarize(m = mean(response)) %>%
  ggplot(aes(x = prob, y = m, color = source)) +
    geom_jitter(alpha = 0.5, width = 0.2) +
    geom_smooth(se = F, method = 'lm', formula = y ~ poly(x,1)) +
    geom_text(aes(label = r, x = -2, y = 0), hjust = 1, data = d.cors) +
    facet_grid(source ~ sent_length, scales = 'free_x' ) +
    theme_few() +
    ylim(-4,100) +
    labs(y = 'naturalness rating', x = 'log probability of sentence') +
    guides(color = F) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggsave('prob-response-relationship.pdf', width = 6, height = 4, units = 'in')
```

