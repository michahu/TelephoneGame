---
title: "R Notebook"
output: html_notebook
---

# Imports

```{r}
library(tidyverse)
library(tidyboot)
library(ggthemes)
library(lme4)
library(lmerTest)
library(reticulate)
library(caret)
library(here)

geom_segment_plus <- function (mapping = NULL, data = NULL, stat = "identity",
  position = "identity", arrow = NULL, lineend = "butt", na.rm = FALSE, ...) {
 
  GeomSegmentPlus$new(mapping = mapping, data = data, stat = stat,
    position = position, arrow = arrow, lineend = lineend, na.rm = na.rm, ...)
}
 
GeomSegmentPlus <- ggproto(ggplot2:::Geom, expr={
  objname <- "segmentplus"
 
  draw <- function(., data, scales, coordinates, arrow = NULL,
    lineend = "butt", na.rm = FALSE, ...) {
 
    data <- remove_missing(data, na.rm = na.rm,
      c("x", "y", "xend", "yend", "linetype", "size", "shape","shorten.start","shorten.end","offset"),
      name = "geom_segment_plus")
    if (empty(data)) return(zeroGrob())
 
    if (is.linear(coordinates)) {
    data = coord_transform(coordinates, data, scales)
      for(i in 1:dim(data)[1] )
      {
        match = data$xend == data$x[i] & data$x == data$xend[i] & data$yend == data$y[i] & data$y == data$yend[i]
        #print("Match:")
        #print(sum(match))
        if( sum( match ) == 0 ) data$offset[i] <- 0
      }
 
      data$dx = data$xend - data$x
      data$dy = data$yend - data$y
      data$dist = sqrt( data$dx^2 + data$dy^2 )
      data$px = data$dx/data$dist
      data$py = data$dy/data$dist
 
      data$x = data$x + data$px * data$shorten.start
      data$y = data$y + data$py * data$shorten.start
      data$xend = data$xend - data$px * data$shorten.end
      data$yend = data$yend - data$py * data$shorten.end
      data$x = data$x - data$py * data$offset
      data$xend = data$xend - data$py * data$offset
      data$y = data$y + data$px * data$offset
      data$yend = data$yend + data$px * data$offset
      
      return(with(data, 
        segmentsGrob(x, y, xend, yend, default.units="native",
        gp = gpar(col=alpha(colour, alpha), fill = alpha(colour, alpha),
          lwd=size * .pt, lty=linetype, lineend = lineend),
        arrow = arrow)
      ))
    }
        print("carrying on")
 
    data$group <- 1:nrow(data)
    starts <- subset(data, select = c(-xend, -yend))
    ends <- rename(subset(data, select = c(-x, -y)), c("xend" = "x", "yend" = "y"),
      warn_missing = FALSE)
    
    pieces <- rbind(starts, ends)
    pieces <- pieces[order(pieces$group),]
    
    GeomPath$draw_groups(pieces, scales, coordinates, arrow = arrow, ...)
  }
 
  
  default_stat <- function(.) StatIdentity
  required_aes <- c("x", "y", "xend", "yend")
  default_aes <- function(.) aes(colour="black", size=0.5, linetype=1, alpha = NA,shorten.start=0,shorten.end=0,offset=0)
  guide_geom <- function(.) "path"
})

```

# Lexical frequency 

## Read in vocab frequency files

Note that reticulate to import pkl might be broken; use `read_csv` instead.

```{r}
gibbs_vocab <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_gibbs_mixture_random_mask_init_1_0.001/VOCABFreqAll.csv"))

mh_vocab <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_mh_mixture_1_random_sweep_mask_init_1_0.001/VOCABFreqAll.csv"))

books_vocab <- read_csv(here("LingFeaturesNew/book/11TokenSents/VOCABFreqAll.csv")) %>% 
  mutate(word = gsub("''", '"', word, fixed = T)) %>%
  group_by(word) %>%
  summarize(count = sum(count))

# need to collapse wikipedia vocab to combine counts for upper-case and lower-case
wiki_vocab <- read_csv(here("LingFeaturesNew/wiki/11TokenSents/VOCABFreqAll.csv")) %>%
  mutate(word = tolower(word)) %>%
  group_by(word) %>%
  summarize(count = sum(count))
```


```{r}
# Note that it matters where we compute the corpus frequencies; 
# do we restrict to just words that appear in bert or the full corpus distribution?
corpus_counts <- wiki_vocab %>%
  filter(count > 2) %>%
  full_join(books_vocab, by = 'word') %>%
  rename(wiki_count = count.x, books_count = count.y) %>%
  replace_na(list(wiki_count = 0, books_count = 0)) %>%
  mutate(corpus_count = wiki_count + books_count,
         corpus_freq = corpus_count / sum(corpus_count))

gibbs_counts <- gibbs_vocab %>%
  rename(gibbs_count = count) %>%
  mutate(gibbs_freq = gibbs_count / sum(gibbs_count))
  
mh_counts <- mh_vocab %>%
  rename(mh_count = count) %>%
  mutate(mh_freq = mh_count / sum(mh_count))

bert_counts <- gibbs_counts %>%
  left_join(mh_counts, by = c('word')) %>%
  filter(mh_count>0)

lexical_counts <- bert_counts %>%
  #filter(!(word %in% c('|', ';', ':'))) %>%
  left_join(corpus_counts, by =c('word')) %>%
  filter(corpus_count>0)
  #replace_na(list(corpus_freq = 1e-8, corpus_count = 0))
  #replace_na(list(gibbs_freq = 1e-8, gibbs_count = 0)) %>%
  #replace_na(list(mh_freq = 1e-8, mh_count = 0))

lexical_ranks <- lexical_counts %>%
  mutate(gibbs_rank = length(gibbs_count) - rank(gibbs_count, ties.method = 'average') + 1,
         mh_rank = length(mh_count) - rank(mh_count, ties.method = 'average') + 1,
         corpus_rank = length(corpus_count) - rank(corpus_count, ties.method = 'average') + 1) %>%
  select(word, gibbs_freq, gibbs_rank, mh_freq, mh_rank, corpus_freq, corpus_rank) %>%
  pivot_longer(cols = c('gibbs_rank', 'mh_rank', 'corpus_rank', 'gibbs_freq','mh_freq', 'corpus_freq'), names_sep = '_', names_to = c('corpus', 'type'))
```

```{r}
lexical_ranks %>%
  pivot_wider(values_from = 'value', names_from = 'type')  %>%
  ggplot(aes(x = rank, y = freq, color = corpus)) +
    geom_line() +
    theme_few() +
    labs(x = '(log) rank of word',
         y ='(log) frequency of word') +
    scale_x_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
    scale_y_log10(labels = scales::trans_format("log10", scales::math_format(10^.x))) +
    theme(aspect.ratio = 1, legend.position = 'top', legend.title = element_blank()) +
    annotation_logticks()

ggsave('zipf-distributions.pdf', width = 4, height =4, units = 'in')
```

```{r}
lexical_ranks %>%
  pivot_wider(values_from = 'value', names_from = 'corpus')  %>%
  filter(type == 'rank') %>%
  ggplot(aes(x = gibbs, y = corpus)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, alpha = 0.2) +
    theme_few() +
    labs(x = 'rank of word (BERT distribution)',
         y ='rank of word (corpus distribution)') +
  theme(aspect.ratio = 1) +
    scale_x_log10(limits = c(.4, 5000), labels = scales::trans_format("log10", scales::math_format(10^.x))) +
    scale_y_log10(limits = c(.4, 5000), labels = scales::trans_format("log10", scales::math_format(10^.x))) +
    annotation_logticks()

ggsave('lexical-distributions_gibbs.pdf', width = 4, height =4, units = 'in')
```
```{r}
lexical_ranks %>%
  pivot_wider(values_from = 'value', names_from = 'corpus')  %>%
  filter(type == 'rank') %>%
  ggplot(aes(x = mh, y = corpus)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, alpha = 0.2) +
    theme_few() +
    labs(x = 'rank of word (BERT distribution)',
         y ='rank of word (corpus distribution)') +
  theme(aspect.ratio = 1) +
    scale_x_log10(limits = c(.4, 5000), labels = scales::trans_format("log10", scales::math_format(10^.x))) +
    scale_y_log10(limits = c(.4, 5000), labels = scales::trans_format("log10", scales::math_format(10^.x))) +
    annotation_logticks()

ggsave('lexical-distributions_mh.pdf', width = 4, height =4, units = 'in')
```

look at correlation

```{r}
lexical_counts %>%
  filter(gibbs_count > 10) %>%
  summarize(c = cor(corpus_count, gibbs_count, method = 'spearman'))
```
```{r}
lexical_counts %>%
  filter(mh_count > 10) %>%
  summarize(c = cor(corpus_count, mh_count, method = 'spearman'))
```

What are words that are common in bert, less common for corpus? 

```{r}
lexical_ranks %>%
  pivot_wider(values_from = 'value', names_from = 'corpus') %>%
  filter(type == 'rank') %>%
  filter(gibbs < 100) %>%
  mutate(diff = gibbs - corpus) %>%
  arrange(diff)
```
```{r}
lexical_ranks %>%
  pivot_wider(values_from = 'value', names_from = 'corpus') %>%
  filter(type == 'rank') %>%
  filter(mh < 100) %>%
  mutate(diff = mh - corpus) %>%
  arrange(diff)
```
What are words that are common for corpus, less common for bert? 

```{r}
lexical_ranks %>%
  pivot_wider(values_from = 'value', names_from = 'corpus') %>%
  filter(type == 'rank') %>%
  filter(corpus < 100) %>%
  mutate(diff = corpus- gibbs) %>%
  arrange(diff)
```
```{r}
lexical_ranks %>%
  pivot_wider(values_from = 'value', names_from = 'corpus') %>%
  filter(type == 'rank') %>%
  filter(corpus < 100) %>%
  mutate(diff = corpus- mh) %>%
  arrange(diff)
```

# POS comparisons

## Read in part of speech frequencies

```{r}
gibbs_POS <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_gibbs_mixture_random_mask_init_1_0.001/POSFreqAll.csv")) %>%
  mutate(prop = count / sum(count)) %>% 
  mutate(source = 'gibbs')

mh_POS <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_mh_mixture_1_random_sweep_mask_init_1_0.001/POSFreqAll.csv")) %>%
  mutate(prop = count / sum(count)) %>% 
  mutate(source = 'mh')

books_POS <- read_csv(here("LingFeaturesNew/book/11TokenSents/POSFreqAll.csv"))
wiki_POS <- read_csv(here("LingFeaturesNew/wiki/11TokenSents/POSFreqAll.csv"))
combined_POS <- books_POS %>%
  full_join(wiki_POS, by = 'word') %>%
  mutate(count = count.x + count.y,
         prop = count / sum(count)) %>%
  select(word,count,prop) %>%
  mutate(source = 'corpus')
```

## Make figure
```{r}
d.arrows <- combined_POS %>% 
  bind_rows(gibbs_POS) %>%
  bind_rows(mh_POS) %>%
  filter(prop > 0.01) %>%
  filter(word != 'PART') %>%
  select(-count) %>%
  pivot_wider(names_from = 'source', values_from = 'prop')
  
d.arrows %>%
  pivot_longer(names_to = 'source', values_to = 'prop', cols = c('gibbs','mh', 'corpus')) %>%
  mutate(word = fct_reorder(word, prop)) %>%
  ggplot(aes(y = word, x = prop, group = word, fill = source)) +
    geom_point(shape=21, size = 3, color = "NA") +
    theme_few() +
    # geom_segment(aes(x=corpus, xend=bert, fill = NA, color = diff > 0, yend = word), 
    #              data=d.arrows, arrow=arrow(angle = 10, type = 'closed', length = unit(0.5, "inches")), 
    #              size=0, alpha = 0.35, lineend = 'round', linejoin = 'round') + 
    theme(aspect.ratio = 1, legend.position = 'top') +
    guides(color=F) +
    labs(y = '', x = 'frequency of part of speech') +
    #scale_fill_manual(values = c('white', 'black','grey')) +
    scale_color_gradient2()

ggsave('pos-distributions.pdf', width = 4, height = 4, units = 'in')
```


```{r}
d.arrows <- combined_POS %>% 
  bind_rows(gibbs_POS) %>%
  filter(prop > 0.01) %>%
  select(-count) %>%
  pivot_wider(names_from = 'source', values_from = 'prop') %>%
  mutate(diff = gibbs - corpus)
  
d.arrows %>%
  pivot_longer(names_to = 'source', values_to = 'prop', cols = c('gibbs', 'corpus')) %>%
  mutate(word = fct_reorder(word, prop)) %>%
  ggplot(aes(y = word, x = prop, group = word, fill = source, color = -diff)) +
    geom_point(shape=21, size = 2, color = 'black') +
    geom_line(alpha = 0.5, size = 3, lineend = 'round') +
    theme_few() +
    # geom_segment(aes(x=corpus, xend=bert, fill = NA, color = diff > 0, yend = word), 
    #              data=d.arrows, arrow=arrow(angle = 10, type = 'closed', length = unit(0.5, "inches")), 
    #              size=0, alpha = 0.35, lineend = 'round', linejoin = 'round') + 
    theme(aspect.ratio = 1, legend.position = 'top') +
    guides(color=F) +
    labs(y = '', x = 'frequency of part of speech') +
    scale_fill_manual(values = c('white', 'black')) +
    scale_color_gradient2()

ggsave('pos-distributions_gibbs.pdf', width = 4, height = 4, units = 'in')
```
```{r}
d.arrows <- combined_POS %>% 
  bind_rows(mh_POS) %>%
  filter(word!='SPACE') %>%
  select(-count) %>%
  pivot_wider(names_from = 'source', values_from = 'prop') %>%
  mutate(diff = mh - corpus)
  
d.arrows %>%
  pivot_longer(names_to = 'source', values_to = 'prop', cols = c('mh', 'corpus')) %>%
  mutate(word = fct_reorder(word, prop)) %>%
  ggplot(aes(y = word, x = prop, group = word, fill = source, color = -diff)) +
    geom_point(shape=21, size = 2, color = 'black') +
    geom_line(alpha = 0.5, size = 3, lineend = 'round') +
    theme_few() +
    # geom_segment(aes(x=corpus, xend=bert, fill = NA, color = diff > 0, yend = word), 
    #              data=d.arrows, arrow=arrow(angle = 10, type = 'closed', length = unit(0.5, "inches")), 
    #              size=0, alpha = 0.35, lineend = 'round', linejoin = 'round') + 
    theme(aspect.ratio = 1, legend.position = 'top') +
    guides(color=F) +
    labs(y = '', x = 'frequency of part of speech') +
    scale_fill_manual(values = c('white', 'black')) +
    scale_color_gradient2()

ggsave('pos-distributions_mh.pdf', width = 4, height = 4, units = 'in')
```

# Dependency comparison

## Import dependency counts
```{r}
gibbs_DEP <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_gibbs_mixture_random_mask_init_1_0.001/DEPFreqAll.csv"))%>%
  filter(!(word %in% c("ROOT", 'det', 'punct','prep', 'aux'))) %>%
  mutate(prop = count / sum(count)) %>% 
  mutate(source = 'gibbs')

mh_DEP <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_mh_mixture_1_random_sweep_mask_init_1_0.001/DEPFreqAll.csv"))%>%
  filter(!(word %in% c("ROOT", 'det', 'punct','prep', 'aux'))) %>%
  mutate(prop = count / sum(count)) %>% 
  mutate(source = 'mh')

books_DEP <- read_csv(here("LingFeaturesNew/book/11TokenSents/DEPFreqAll.csv"))
wiki_DEP <- read_csv(here("LingFeaturesNew/wiki/11TokenSents/DEPFreqAll.csv"))

combined_DEP <- books_DEP %>%
  full_join(wiki_DEP, by = 'word') %>%
  filter(!(word %in% c("ROOT", 'det', 'punct','prep', 'aux'))) %>%
  mutate(count = count.x + count.y,
         prop = count / sum(count)) %>%
  select(word,count,prop) %>%
 mutate(source = 'corpus')
```

## Make figure

```{r}
d.arrows.dep <- combined_DEP %>% 
  bind_rows(gibbs_DEP) %>%
  bind_rows(mh_DEP) %>%
  select(-count) %>%
  pivot_wider(names_from = 'source', values_from = 'prop') %>%
  filter(corpus > 0.01) 
  
d.arrows.dep %>%
  pivot_longer(names_to = 'source', values_to = 'prop', cols = c('gibbs','mh', 'corpus')) %>%
  mutate(word = fct_reorder(word, prop)) %>%
  ggplot(aes(y = word, x = prop, group = word, fill = source)) +
    #geom_line(alpha = 0.5, size = 3, lineend = 'round') +
    theme_few() +
    # geom_segment_plus(aes(x=corpus, xend=bert, fill = NA, color = -diff, yend = word),
    #              data=d.arrows.dep, arrow=arrow(angle = 30, type = 'closed', length = unit(0.05, "inches")),
    #              size=1, alpha = 1, lineend = 'butt', linejoin = 'round') +
    geom_point(shape=21, size = 3, color="NA") +
    theme(aspect.ratio = 1, legend.position = 'top') +
    guides(color=F) +
    #xlim(0, .188) +
    labs(y = '', x = 'frequency of dependency') +
    #scale_fill_manual(values = c('white','black','grey')) +
    scale_color_gradient2()

ggsave('dep-distributions.pdf', width = 4, height =4, units = 'in')
```


# Dependency distance comparison

## Import dependency distances

```{r}
gibbs_DEP_DIST <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_gibbs_mixture_random_mask_init_1_0.001/DEP_DISTFreqAll.csv")) %>% 
  mutate(source = 'gibbs') %>%
  mutate(prop = count / sum(count)) %>%
  mutate(word = as.numeric(word)) 
mh_DEP_DIST <- read_csv(here("LingFeaturesNew/bert/11TokenSents/5_51000_500_mh_mixture_1_random_sweep_mask_init_1_0.001/DEP_DISTFreqAll.csv")) %>%
  mutate(source = 'mh') %>%
  mutate(prop = count / sum(count)) %>%
  mutate(word = as.numeric(word))
  
books_DEP_DIST <- read_csv(here("LingFeaturesNew/book/11TokenSents/DEP_DISTFreqAll.csv")) %>%
  mutate(source = 'book') %>%
    mutate(word = as.numeric(word)) %>%
  mutate(prop = count / sum(count))
wiki_DEP_DIST <- read_csv(here("LingFeaturesNew/wiki/11TokenSents/DEP_DISTFreqAll.csv")) %>%
  mutate(source = 'wiki') %>%
    mutate(word = as.numeric(word)) %>%
  mutate(prop = count / sum(count))

combined_DEP_DIST <- books_DEP_DIST %>%
  full_join(wiki_DEP_DIST, by = 'word') %>%
  replace_na(list(count.y = 0, count.x = 0)) %>%
  mutate(count = count.x + count.y,
         prop = count / sum(count)) %>%
  dplyr::select(word,count,prop) %>%
   mutate(source = 'combined\ncorpora')%>%
  ungroup()
```

## Make figures

```{r}
combined_DEP_DIST %>%
  bind_rows(books_DEP_DIST) %>%
  bind_rows(wiki_DEP_DIST) %>%
  bind_rows(gibbs_DEP_DIST) %>%
  bind_rows(mh_DEP_DIST) %>%
  #group_by(source) %>%
  filter(word < 40) %>%
  ggplot(aes(x = word, fill = source, weight = prop)) +
    geom_histogram(aes(y=..density..), position = 'identity', bins = 30, alpha = 0.5) +
    geom_density(position="identity", alpha = 0.3, adjust =.4) +
    facet_wrap(~factor(source,levels=c('wiki','book','combined\ncorpora','gibbs','mh'))) +
    theme_few() +
    theme(aspect.ratio = 1, legend.position = 'none') 
ggsave('dep_dist_density.pdf', width = 4, height = 4, units = 'in')
```

```{r}
combined_DEP_DIST %>%
  bind_rows(books_DEP_DIST) %>%
  bind_rows(wiki_DEP_DIST) %>%
  bind_rows(gibbs_DEP_DIST) %>%
  bind_rows(mh_DEP_DIST) %>%
  filter(word < 75) %>%
  ggplot(aes(x = as.numeric(word), fill = source, weight = prop)) +
    #geom_histogram(aes(y=..density..), position = 'identity', bins = 20, alpha = 0.5) +
    geom_density(position="identity", alpha = 0.3, adjust = 0.4) +
    labs(x = 'dependency length') +
    theme_few() +
    facet_wrap(~ source)
    theme(aspect.ratio = 1, legend.position = 'top', legend.title = element_blank()) 

ggsave('dep_dist_density.pdf', width = 4, height = 5, units = 'in')
```

## Plot CDFs

```{r}
cdfs <- combined_DEP_DIST %>%
  bind_rows(gibbs_DEP_DIST) %>%
  bind_rows(mh_DEP_DIST) %>%
  filter(word < 30) %>%
  group_by(source) %>%
  arrange(word,.by_group=TRUE) %>% 
  mutate(cdf = cumsum(prop)) %>%
  ungroup() 

cdfs %>%
  ggplot(aes(x = as.numeric(word), color = source, y = cdf)) +
    geom_line(size=1.5) +
    #geom_density(position="identity", alpha = 0.3, adjust = 0.35) +
    labs(x = 'dependency length') +
    theme_few() +
    theme(aspect.ratio = 1, legend.position = 'top', legend.title = element_blank()) 

ggsave('dep_dist.pdf', width = 4, height = 5, units = 'in')
```

KS statistic (i.e. max difference between cdf curves)

```{r}
cdfs %>%
  select(-count, -prop) %>%
  spread(source, cdf) %>%
  drop_na(everything()) %>%
  mutate(gibbs_diff = abs(gibbs - `combined corpora`)) %>%
  filter(gibbs_diff == max(gibbs_diff))
```

KS test

```{r}
gibbs_vals = cdfs %>% filter(source == 'gibbs') %>% do(data.frame(val = rep(.$word, .$count))) %>% pull(val)
corpus_vals = cdfs %>% filter(source == 'combined corpora') %>% do(data.frame(val = rep(.$word, .$count))) %>% pull(val)
ks.test(gibbs_vals, corpus_vals, alternative = 'greater')
```
```{r}
cdfs %>%
  select(-count, -prop) %>%
  spread(source, cdf) %>%
  drop_na(everything()) %>%
  mutate(mh_diff = abs(mh - `combined corpora`)) %>%
  filter(mh_diff == max(mh_diff))
```
```{r}
mh_vals = cdfs %>% filter(source == 'mh') %>% do(data.frame(val = rep(.$word, .$count))) %>% pull(val)
corpus_vals = cdfs %>% filter(source == 'combined corpora') %>% do(data.frame(val = rep(.$word, .$count))) %>% pull(val)
ks.test(mh_vals, corpus_vals, alternative = 'greater')
```


???

```{r}
bert_DEP_DIST %>%
  bind_rows(combined_DEP_DIST) %>%
  group_by(source) %>% 
  filter(count == max(count))
  # replace_na(list(prop = 0)) %>%
  # arrange(word) %>%
  # filter(word < 70) %>%
  # summarize(mean(as.numeric(word) * prop, rm.na = T)) 
```
